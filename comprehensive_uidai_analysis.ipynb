{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# UIDAI National Hackathon: Strategic Insights & Advanced Analytics\n",
        "## Author: Senior Data Scientist, UIDAI\n",
        "\n",
        "This notebook consolidates the entire analytical pipeline for the Aadhaar enrolment and update datasets (March - December 2025). It includes data cleaning, multi-layer EDA, clustering analysis, and the Service Stress Index (SSI) along with advanced visualizations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Setup & Environment Initialisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pandas'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[34;01mplt\u001b[39;00m\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import os\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Aesthetic configuration\n",
        "plt.style.use('bmh')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 11\n",
        "\n",
        "base_dir = '.'  # Project root for GitHub portability\n",
        "visuals_dir = os.path.join(base_dir, 'visuals')\n",
        "os.makedirs(visuals_dir, exist_ok=True)\n",
        "print('Environment Ready.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Data Consolidation & Cleaning\n",
        "Merging multiple CSV chunks for enrolment, biometric, and demographic updates into unified DataFrames."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_and_clean(category_folder, date_format='%d-%m-%Y'):\n",
        "    folder_path = os.path.join(base_dir, category_folder)\n",
        "    files = glob.glob(os.path.join(folder_path, '*.csv'))\n",
        "    df = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)\n",
        "    df['date'] = pd.to_datetime(df['date'], format=date_format)\n",
        "    df['state'] = df['state'].str.strip().str.upper()\n",
        "    df['district'] = df['district'].str.strip().str.upper()\n",
        "    return df\n",
        "\n",
        "en_df = load_and_clean('api_data_aadhar_enrolment (2)')\n",
        "bio_df = load_and_clean('api_data_aadhar_biometric')\n",
        "demo_df = load_and_clean('api_data_aadhar_demographic')\n",
        "\n",
        "print(f'Enrolment Records: {len(en_df)}')\n",
        "print(f'Biometric Records: {len(bio_df)}')\n",
        "print(f'Demographic Records: {len(demo_df)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Feature Engineering & Multi-Dimensional Aggregation\n",
        "Creating a master daily state summary and derived metrics like the Update-to-Enrolment ratio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "en_df['total_enrolment'] = en_df[['age_0_5', 'age_5_17', 'age_18_greater']].sum(axis=1)\n",
        "bio_df['total_biometric'] = bio_df[['bio_age_5_17', 'bio_age_17_']].sum(axis=1)\n",
        "demo_df['total_demographic'] = demo_df[['demo_age_5_17', 'demo_age_17_']].sum(axis=1)\n",
        "\n",
        "en_agg = en_df.groupby(['date', 'state']).agg({'total_enrolment': 'sum'}).reset_index()\n",
        "bio_agg = bio_df.groupby(['date', 'state']).agg({'total_biometric': 'sum'}).reset_index()\n",
        "demo_agg = demo_df.groupby(['date', 'state']).agg({'total_demographic': 'sum'}).reset_index()\n",
        "\n",
        "master_agg = en_agg.merge(bio_agg, on=['date', 'state'], how='outer')\n",
        "master_agg = master_agg.merge(demo_agg, on=['date', 'state'], how='outer').fillna(0)\n",
        "\n",
        "master_agg['month'] = master_agg['date'].dt.strftime('%Y-%m')\n",
        "master_agg['total_updates'] = master_agg['total_biometric'] + master_agg['total_demographic']\n",
        "master_agg['total_activity'] = master_agg['total_enrolment'] + master_agg['total_updates']\n",
        "\n",
        "print('Master aggregation completed.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Advanced Clustering Analysis\n",
        "Grouping states based on their operational behavior (Enrolment vs. Biometric vs. Demographic dominant)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "state_behavior = master_agg.groupby('state').agg({\n",
        "    'total_enrolment': 'sum',\n",
        "    'total_biometric': 'sum',\n",
        "    'total_demographic': 'sum',\n",
        "    'total_activity': 'sum'\n",
        "}).reset_index()\n",
        "\n",
        "state_behavior = state_behavior[state_behavior['total_activity'] > 5000].copy()\n",
        "X = state_behavior[['total_enrolment', 'total_biometric', 'total_demographic']]\n",
        "X_norm = X.div(X.sum(axis=1), axis=0)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_norm)\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "state_behavior['cluster'] = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "# NEW: Clustering Visualization\n",
        "plt.figure(figsize=(10, 8))\n",
        "scatter = plt.scatter(X_norm['total_enrolment'], X_norm['total_biometric'], \n",
        "            c=state_behavior['cluster'], cmap='viridis', s=100, alpha=0.7, edgecolors='k')\n",
        "plt.xlabel('Proportion of Enrolment')\n",
        "plt.ylabel('Proportion of Biometric Updates')\n",
        "plt.title('State Clusters: Operational Archetypes', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Add labels for key states\n",
        "for i, txt in enumerate(state_behavior['state']):\n",
        "    if state_behavior.iloc[i]['total_activity'] > 1000000:\n",
        "        plt.annotate(txt, (X_norm.iloc[i]['total_enrolment'], X_norm.iloc[i]['total_biometric']), fontsize=9)\n",
        "\n",
        "plt.colorbar(scatter, label='Cluster ID')\n",
        "plt.savefig(os.path.join(visuals_dir, 'state_clusters_scatter.png'), dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Service Stress Index (SSI)\n",
        "Predictive modeling to identify infrastructure hotspots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "master_clean = master_agg[master_agg['date'].dt.day != 1].copy()\n",
        "state_metrics = master_clean.groupby('state').agg({\n",
        "    'total_enrolment': 'sum',\n",
        "    'total_biometric': 'sum',\n",
        "    'total_demographic': 'sum',\n",
        "    'total_updates': 'sum'\n",
        "}).reset_index()\n",
        "\n",
        "state_metrics['ratio'] = state_metrics['total_updates'] / (state_metrics['total_enrolment'] + 10)\n",
        "daily_vol = master_clean.groupby('state').apply(lambda x: x['total_updates'].std()).reset_index(name='volatility')\n",
        "state_metrics = state_metrics.merge(daily_vol, on='state')\n",
        "state_metrics['vol_index'] = state_metrics['volatility'] / (state_metrics['total_updates'] / 300 + 1)\n",
        "\n",
        "def norm(s): return 100 * (s - s.min()) / (s.max() - s.min() + 1e-6)\n",
        "state_metrics['SSI'] = (norm(state_metrics['ratio']) * 0.5) + (norm(state_metrics['vol_index']) * 0.5)\n",
        "state_metrics = state_metrics[state_metrics['total_enrolment'] > 1000].sort_values('SSI', ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.barh(state_metrics.head(15)['state'][::-1], state_metrics.head(15)['SSI'][::-1], color='maroon')\n",
        "plt.title('Critical Hotspots: Service Stress Index', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Stress Score (0-100)')\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(visuals_dir, 'final_ssi_chart.png'), dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. NEW: Geographic Demand Concentration (Lorenz Curve)\n",
        "Visualising the inequality of UIDAI service demands across states. A high curvature indicates that service demand is concentrated in a few super-active nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_lorenz_curve(data, label, color):\n",
        "    sorted_data = np.sort(data)\n",
        "    cum_data = np.cumsum(sorted_data) / np.sum(sorted_data)\n",
        "    cum_pts = np.linspace(0, 1, len(cum_data))\n",
        "    plt.plot(cum_pts, cum_data, label=label, color=color, linewidth=3)\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plot_lorenz_curve(state_metrics['total_updates'], 'Updates Demand', 'orange')\n",
        "plot_lorenz_curve(state_metrics['total_enrolment'], 'Enrolment Demand', 'green')\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Perfect Equality')\n",
        "plt.title('Geographic Demand Concentration: Enrolment vs Updates', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Cumulative Proportion of States')\n",
        "plt.ylabel('Cumulative Proportion of Demand')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig(os.path.join(visuals_dir, 'lorenz_concentration.png'), dpi=300)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
